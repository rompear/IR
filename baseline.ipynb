{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import h5py\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from heapq import nlargest\n",
    "import dill\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import PIL\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "print(CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_FEATURES = \"./images/IR_image_features.h5\"\n",
    "IMG_ID = \"./images/IR_img_features2id.json\"\n",
    "\n",
    "TRAIN_HARD = \"./Data/Hard/IR_train_hard.json\"\n",
    "TRAIN_EASY = \"./Data/Easy/IR_train_easy.json\"\n",
    "TEST_HARD = \"./Data/Hard/IR_test_hard.json\"\n",
    "TEST_EASY = \"./Data/Easy/IR_test_easy.json\"\n",
    "VAL_HARD = \"./Data/Hard/IR_val_hard.json\"\n",
    "VAL_EASY = \"./Data/Easy/IR_val_easy.json\"\n",
    "\n",
    "IMGID2IMGINFO = \"./Data/imgid2imginfo.json\"\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GET features from images\n",
    "with open(\"./images/IR_img_features2id.json\", 'r') as f:\n",
    "    visual_feat_mapping = json.load(f)['IR_imgid2id']\n",
    "f.close()\n",
    "\n",
    "img_features = np.asarray( h5py.File(\"./images/IR_image_features.h5\", 'r')['img_features'])\n",
    "\n",
    "def get_feature_from_id(img_id):\n",
    "    h5_id = visual_feat_mapping[str(img_id)]\n",
    "    return img_features[h5_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Helper function to show an image, from a image id \"\"\"\n",
    "def show_img_from_id(img_ids, target_id = -1):\n",
    "    with open(IMGID2IMGINFO, 'r') as f:\n",
    "        imgid2info = json.load(f)\n",
    "    \n",
    "    imgs = []\n",
    "    for img_id in img_ids:\n",
    "        response = requests.get(imgid2info[str(img_id)]['coco_url'])\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        imgs.append(img)\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n",
    "    imgs_comb = np.hstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "\n",
    "    # save that beautiful picture\n",
    "    imgs_comb = PIL.Image.fromarray( imgs_comb)\n",
    "    imgs_comb.show()  \n",
    "    # imgs_comb.save( 'Trifecta_vertical.jpg' )\n",
    "    \n",
    "show_img_from_id([378466, 378466, 378466, 378466])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocess a sentence\n",
    "\"\"\"\n",
    "def preprocess(sentence, stemmer, stop):\n",
    "    low_sent = sentence.lower()\n",
    "    return word_tokenize(low_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert Samples\n",
    "\"\"\"\n",
    "def get_dialog_caption_targets_from_sample(sample):\n",
    "    dialog = ''\n",
    "    caption = sample['caption']\n",
    "    targets = []\n",
    "    targetidx = sample['target']\n",
    "    for d in sample['dialog']:\n",
    "        dialog += ' ' + d[0]\n",
    "    for img in sample['img_list']:\n",
    "        targets.append(img)\n",
    "    return dialog, caption, targets, targetidx\n",
    "\n",
    "Sample = namedtuple(\"Sample\", [\"words\", \"images\", \"target\"])\n",
    "\n",
    "\"\"\"\n",
    " For every Sample we retrieve the sentences and the img_ids, and the correct target_id. \n",
    "\"\"\"\n",
    "def read_dataset(filename, stemmer, stopwords):\n",
    "    with open(filename, \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "    f.close()\n",
    "    for idx, sample in enumerate(dataset):\n",
    "        if(idx % 10000 == 0):\n",
    "            print(idx)\n",
    "        dialog, caption, targets, targetidx = get_dialog_caption_targets_from_sample(dataset[\n",
    "                                                                                     str(sample)])\n",
    "        sentences = preprocess(dialog + ' ' + caption, stemmer, stopwords)\n",
    "        yield Sample(words=[w2i[x] for x in sentences], images=targets, target=targetidx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))\n",
    "UNK = w2i[\"<UNK>\"]\n",
    "PAD = w2i[\"<PAD>\"]\n",
    "\n",
    "# Do this super one time \n",
    "import nltk\n",
    "# nltk.download(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop = stopwords.words('english') + list(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#read the datasets and use w2i (only do this once)\n",
    "train = list(read_dataset(TRAIN_HARD, stemmer, stop))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "val = list(read_dataset(VAL_HARD, stemmer, stop))\n",
    "test = list(read_dataset(TEST_HARD, stemmer, stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nwords = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CLASS LSTM \n",
    "\"\"\"\n",
    "\n",
    "class ClassificationNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, img_feat_dim, hidden_dim_mlp, output_dim, batch_size):\n",
    "        super(ClassificationNN,self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=PAD)\n",
    "        self.batch_size = batch_size\n",
    "        self.linear1 = nn.Linear(embedding_size+img_feat_dim,hidden_dim_mlp)\n",
    "        self.linear2 = nn.Linear(hidden_dim_mlp,output_dim)\n",
    "\n",
    "    def forward(self, sentence, image_feat):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = torch.sum(embeds, 1)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.repeat(1,10,1)\n",
    "        lin1 = F.sigmoid(self.linear1(torch.cat((x, image_feat),2)))\n",
    "        lin2 = self.linear2(lin1)\n",
    "        return lin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_size:  100 LEARNING_RATE:  0.001\n",
      "\n",
      "ClassificationNN (\n",
      "  (word_embeddings): Embedding(21999, 300, padding_idx=1)\n",
      "  (linear1): Linear (2348 -> 48)\n",
      "  (linear2): Linear (48 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# INIT MODEL AND INIT OPTIMIZER\n",
    "print(\"Batch_size: \", BATCH_SIZE, \"LEARNING_RATE: \",LEARNING_RATE)\n",
    "print()\n",
    "\n",
    "model = ClassificationNN(nwords, 300, 2048, 48, 1, BATCH_SIZE)\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "print(model)\n",
    "\n",
    "#@TODO we can use a adaptive learnrate for adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HELPER FUNCTIONS\n",
    "\"\"\"\n",
    "def preprocessbatch(batch):\n",
    "    \"\"\" Add zero-padding to a batch. \"\"\"\n",
    "    seqs = [sample.words for sample in batch]\n",
    "    max_length = max(map(len, seqs))\n",
    "    seqs = [seq + [PAD] * (max_length - len(seq)) for seq in seqs]\n",
    "\n",
    "    ims = np.array([[get_feature_from_id(img_id) for img_id in sample.images] for sample in batch])\n",
    "\n",
    "    idxs = [sample.target for sample in batch]\n",
    "    \n",
    "    image_ids = [sample.images for sample in batch]\n",
    "\n",
    "    return seqs, ims, idxs, image_ids\n",
    "\n",
    "def minibatch(data, batch_size=BATCH_SIZE):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size]\n",
    "\n",
    "def getLongTensor(x):\n",
    "    tensor = torch.cuda.LongTensor(x) if CUDA else torch.LongTensor(x)\n",
    "    return Variable(tensor)\n",
    "\n",
    "\n",
    "def getFloatTensor(x):\n",
    "    tensor = torch.cuda.FloatTensor(torch.from_numpy(\n",
    "        x).cuda()) if CUDA else torch.FloatTensor(x)\n",
    "    return Variable(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "FUNCTIONS TO SEE HOW GOOD OUR MODEL DID\n",
    "\n",
    "@TODO: Not fast \n",
    "@TODO: top 5 currently not implemented\n",
    "\"\"\"\n",
    "def evaluate(model, data, show_n_images = 0):\n",
    "    top1 = 0\n",
    "    top5 = 0\n",
    "    \n",
    "    counter = 0\n",
    "    for batch in minibatch(data):\n",
    "        seqs, image_features, idxs, image_ids = preprocessbatch(batch)\n",
    "        scores = model(getLongTensor(seqs), getFloatTensor(image_features))\n",
    "        targets = getLongTensor([idxs])\n",
    "        _, predictions = torch.max(scores[:, :,0].data, 1)\n",
    "        \n",
    "\n",
    "        top1 += torch.eq(predictions, targets[0]).sum().data[0]\n",
    "        \n",
    "        _, top5_predictions = torch.topk(scores[:, :,0].data, 5)\n",
    "        _, top10_predictions = torch.topk(scores[:, :,0].data, 10)\n",
    "        for i in range(len(top5_predictions)):\n",
    "            if(targets[0][i].data.numpy() in top5_predictions[i].numpy()):\n",
    "                top5 += 1\n",
    "                \n",
    "                #Show the images Good examples\n",
    "                if counter < show_n_images:\n",
    "                    image_list = [int(np.array(image_ids[i])[idxs[i]])]\n",
    "                    image_list += list(np.array(image_ids[i])[top10_predictions[i].numpy()])\n",
    "                    show_img_from_id(image_list)\n",
    "                    counter += 1\n",
    "            else: \n",
    "                #Show the images Bad examples\n",
    "                if counter < show_n_images:\n",
    "                    image_list = [int(np.array(image_ids[i])[idxs[i]])]\n",
    "                    image_list += list(np.array(image_ids[i])[top10_predictions[i].numpy()])\n",
    "                    show_img_from_id(image_list)\n",
    "                    counter += 1\n",
    "                \n",
    "           \n",
    "    return top1/len(data), top5/len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update: 100, train_loss: 2.291453647613525, time 16.41653084754944\n",
      "update: 200, train_loss: 2.291338292360306, time 31.745835065841675\n",
      "update: 300, train_loss: 2.2917072065671285, time 46.49011301994324\n",
      "update: 400, train_loss: 2.291738773584366, time 61.255143880844116\n",
      "iter 0: avg train loss=2.2917, time=61.26s\n",
      "TOP 1: 0.0976, TOP 5: 0.5644 \n",
      "\n",
      "update: 100, train_loss: 2.2983917593955994, time 15.716618061065674\n",
      "update: 200, train_loss: 2.2974479007720947, time 31.559258937835693\n",
      "update: 300, train_loss: 2.295619272391001, time 48.363394021987915\n",
      "update: 400, train_loss: 2.2946953344345093, time 65.081218957901\n",
      "iter 1: avg train loss=2.2947, time=65.08s\n",
      "TOP 1: 0.1096, TOP 5: 0.583 \n",
      "\n",
      "update: 100, train_loss: 2.288473765850067, time 16.144151210784912\n",
      "update: 200, train_loss: 2.2863420820236207, time 33.487898111343384\n",
      "update: 300, train_loss: 2.2884140841166176, time 52.32359600067139\n",
      "update: 400, train_loss: 2.2881319373846054, time 70.17048621177673\n",
      "iter 2: avg train loss=2.2881, time=70.17s\n",
      "TOP 1: 0.1156, TOP 5: 0.5824 \n",
      "\n",
      "update: 100, train_loss: 2.2798001551628113, time 17.508409023284912\n",
      "update: 200, train_loss: 2.2804708003997805, time 34.8799991607666\n",
      "Stopped at ITER: 3\n",
      "TOP 1: 0.1158, TOP 5: 0.5928\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    RUNNING THE MODEL!!!!!!!!!!\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    for ITER in range(EPOCHS):\n",
    "        # Init variable\n",
    "        random.shuffle(train)\n",
    "        train_loss = 0.0\n",
    "        start = time.time()\n",
    "        updates = 0\n",
    "        \n",
    "        for batch in minibatch(train):\n",
    "            updates += 1\n",
    "            \n",
    "            # pad data with zeros\n",
    "            seqs, image_features, idxs, _ = preprocessbatch(batch)\n",
    "            \n",
    "            #reset hidden layer.1\n",
    "            #@todo not certain if we need this\n",
    "            #  model.hidden = model.init_hidden() \n",
    "            \n",
    "            \n",
    "            # forward pass\n",
    "            scores = model(getLongTensor([seqs])[0], getFloatTensor(image_features))\n",
    "            targets = getLongTensor([idxs])\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            output = loss(scores[:, :, 0], targets[0])\n",
    "            train_loss += output.data[0]\n",
    "            \n",
    "            # backward pass\n",
    "            model.zero_grad()\n",
    "            output.backward()\n",
    "            \n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if(updates % 100 == 0):\n",
    "                print(\"update: {}, train_loss: {}, time {}\".format(updates, train_loss/updates, time.time()-start))\n",
    "            \n",
    "        print(\"iter %r: avg train loss=%.4f, time=%.2fs\" % (ITER, train_loss/updates, time.time()-start))\n",
    "        top1, top5 = evaluate(model, val)   \n",
    "        print(\"TOP 1: {}, TOP 5: {} \\n\".format(top1, top5))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Stopped at ITER: ' + str(ITER))\n",
    "\n",
    "top1, top5 = evaluate(model, test, show_n_images=0)\n",
    "print(\"TOP 1: {}, TOP 5: {}\".format(top1, top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
